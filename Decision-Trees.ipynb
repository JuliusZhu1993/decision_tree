{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !/usr/bin/env python\n",
    "#  -*- coding: utf-8 -*-\n",
    "# __author__ == \"Julius\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  决策树 不加剪枝的  计算信息增益  ID3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "['长', '短', '短', '长', '短', '短', '长', '长']\n",
      "0\n",
      "1\n",
      "['粗', '粗', '粗', '细', '细', '粗', '粗', '粗']\n",
      "1\n",
      "0\n",
      "['长', '短', '短', '短', '长', '长']\n",
      "0\n",
      "{'声音': {'粗': {'头发': {'长': '女', '短': '男'}}, '细': '女'}}\n"
     ]
    }
   ],
   "source": [
    "#所有数据熵的总和\n",
    "def calcShannonEnt(dataSet):  # 计算数据的熵(entropy)\n",
    "    numEntries=len(dataSet)  # 数据条数\n",
    "    labelCounts={}\n",
    "    for featVec in dataSet:\n",
    "        currentLabel=featVec[-1] # 每行数据的最后一个字（类别）\n",
    "        if currentLabel not in labelCounts.keys():\n",
    "            labelCounts[currentLabel]=0\n",
    "        labelCounts[currentLabel]+=1  # 统计有多少个类以及每个类的数量\n",
    "    shannonEnt=0\n",
    "    for key in labelCounts:\n",
    "        prob=float(labelCounts[key])/numEntries # 计算单个类的熵值\n",
    "        shannonEnt-=prob*log(prob,2) # 累加每个类的熵值\n",
    "    return shannonEnt\n",
    "\n",
    "def createDataSet1():    # 创造示例数据\n",
    "    dataSet = [['长', '粗', '男'],\n",
    "               ['短', '粗', '男'],\n",
    "               ['短', '粗', '男'],\n",
    "               ['长', '细', '女'],\n",
    "               ['短', '细', '女'],\n",
    "               ['短', '粗', '女'],\n",
    "               ['长', '粗', '女'],\n",
    "               ['长', '粗', '女']]\n",
    "    labels = ['头发','声音']  #两个特征\n",
    "    return dataSet,labels\n",
    "\n",
    "def splitDataSet(dataSet,axis,value): # 按某个特征分类后的数据\n",
    "    retDataSet=[]\n",
    "    for featVec in dataSet:\n",
    "        if featVec[axis]==value:\n",
    "            reducedFeatVec =featVec[:axis]\n",
    "            reducedFeatVec.extend(featVec[axis+1:])\n",
    "            retDataSet.append(reducedFeatVec)\n",
    "    return retDataSet\n",
    "\n",
    "def chooseBestFeatureToSplit(dataSet):  # 选择最优的分类特征\n",
    "    numFeatures = len(dataSet[0])-1\n",
    "    baseEntropy = calcShannonEnt(dataSet)  # 原始的熵\n",
    "    bestInfoGain = 0\n",
    "    bestFeature = -1\n",
    "    for i in range(numFeatures):\n",
    "        print(i)\n",
    "        featList = [example[i] for example in dataSet]\n",
    "        print(featList)\n",
    "        uniqueVals = set(featList)\n",
    "        newEntropy = 0\n",
    "        for value in uniqueVals:\n",
    "            subDataSet = splitDataSet(dataSet,i,value)\n",
    "            prob =len(subDataSet)/float(len(dataSet))\n",
    "            newEntropy +=prob*calcShannonEnt(subDataSet)  # 按特征分类后的熵\n",
    "        infoGain = baseEntropy - newEntropy  # 原始熵与按特征分类后的熵的差值\n",
    "        if (infoGain>bestInfoGain):   # 若按某特征划分后，熵值减少的最大，则次特征为最优分类特征\n",
    "            bestInfoGain=infoGain\n",
    "            bestFeature = i\n",
    "        print(bestFeature)\n",
    "    return bestFeature\n",
    "\n",
    "def majorityCnt(classList):    #按分类后类别数量排序，比如：最后分类为2男1女，则判定为男；\n",
    "    classCount={}\n",
    "    for vote in classList:\n",
    "        if vote not in classCount.keys():\n",
    "            classCount[vote]=0\n",
    "        classCount[vote]+=1\n",
    "    sortedClassCount = sorted(classCount.items(),key=operator.itemgetter(1),reverse=True)\n",
    "    return sortedClassCount[0][0]\n",
    "\n",
    "def createTree(dataSet,labels):\n",
    "    classList=[example[-1] for example in dataSet]  # 类别：男或女\n",
    "    if classList.count(classList[0])==len(classList):\n",
    "        return classList[0]\n",
    "    if len(dataSet[0])==1:\n",
    "        return majorityCnt(classList)\n",
    "    bestFeat=chooseBestFeatureToSplit(dataSet) #选择最优特征\n",
    "    bestFeatLabel=labels[bestFeat]\n",
    "    myTree={bestFeatLabel:{}} #分类结果以字典形式保存\n",
    "    del(labels[bestFeat])\n",
    "    featValues=[example[bestFeat] for example in dataSet]\n",
    "    uniqueVals=set(featValues)\n",
    "    for value in uniqueVals:\n",
    "        subLabels=labels[:]\n",
    "        myTree[bestFeatLabel][value]=createTree(splitDataSet\\\n",
    "                            (dataSet,bestFeat,value),subLabels)\n",
    "    return myTree\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    dataSet, labels=createDataSet1()  # 创造示列数据\n",
    "    print(createTree(dataSet, labels))  # 输出决策树模型结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 基于skl-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "iris_feature = iris.data\n",
    "iris_target = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train, feature_test, target_train, target_test = train_test_split(iris_feature, iris_target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt_model = DecisionTreeClassifier()\n",
    "dt_model.fit(feature_train,target_train)\n",
    "predict_results = dt_model.predict(feature_test)\n",
    "scores = dt_model.score(feature_test, target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2, 0,\n",
       "       2, 2, 2, 2, 2, 0, 0])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
